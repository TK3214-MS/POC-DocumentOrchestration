{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概要説明\n",
    "\n",
    "これは、特定のロジックをテストするためだけに完全な Azure 関数を実行するという厳密さを伴わずに、Azure 関数のコンポーネントを簡単にテストするためのノートブックです。これは単体テストに代わるものではなく、Azure 関数を使用して手動で最新の状態に保つ必要がありますが、ロジックのユニットをすばやく試すのに便利です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure.ai.documentintelligence in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 5)) (1.0.0b2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: azure-ai-textanalytics in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 6)) (5.3.0)\n",
      "Requirement already satisfied: azure.core in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 7)) (1.30.1)\n",
      "Requirement already satisfied: azure-functions in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 8)) (1.18.0)\n",
      "Requirement already satisfied: azure-functions-durable in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 9)) (1.2.9)\n",
      "Requirement already satisfied: langchain in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 10)) (0.1.11)\n",
      "Requirement already satisfied: langdetect in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 11)) (1.0.9)\n",
      "Requirement already satisfied: python-dotenv in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 12)) (1.0.1)\n",
      "Requirement already satisfied: semantic_kernel in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 13)) (0.9.1b1)\n",
      "Requirement already satisfied: azure.storage.blob in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 14)) (12.19.1)\n",
      "Requirement already satisfied: azure.ai.formrecognizer in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from -r ../src/requirements.txt (line 15)) (3.3.2)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.6.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure.ai.documentintelligence->-r ../src/requirements.txt (line 5)) (0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure.ai.documentintelligence->-r ../src/requirements.txt (line 5)) (4.10.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure-ai-textanalytics->-r ../src/requirements.txt (line 6)) (1.1.28)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure.core->-r ../src/requirements.txt (line 7)) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure.core->-r ../src/requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: aiohttp>=3.6.2 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure-functions-durable->-r ../src/requirements.txt (line 9)) (3.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure-functions-durable->-r ../src/requirements.txt (line 9)) (2.9.0.post0)\n",
      "Requirement already satisfied: furl>=2.1.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure-functions-durable->-r ../src/requirements.txt (line 9)) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from requests>=2.21.0->azure.core->-r ../src/requirements.txt (line 7)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from requests>=2.21.0->azure.core->-r ../src/requirements.txt (line 7)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from requests>=2.21.0->azure.core->-r ../src/requirements.txt (line 7)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from requests>=2.21.0->azure.core->-r ../src/requirements.txt (line 7)) (2024.2.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (2.0.28)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (0.0.27)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.29 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (0.1.30)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (0.1.23)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (2.6.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain->-r ../src/requirements.txt (line 10)) (8.2.3)\n",
      "Requirement already satisfied: aiofiles<24.0.0,>=23.1.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from semantic_kernel->-r ../src/requirements.txt (line 13)) (23.2.1)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from semantic_kernel->-r ../src/requirements.txt (line 13)) (0.7.1)\n",
      "Requirement already satisfied: motor<4.0.0,>=3.3.2 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from semantic_kernel->-r ../src/requirements.txt (line 13)) (3.3.2)\n",
      "Requirement already satisfied: openai>=1.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from semantic_kernel->-r ../src/requirements.txt (line 13)) (1.13.3)\n",
      "Requirement already satisfied: openapi_core<0.19.0,>=0.18.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from semantic_kernel->-r ../src/requirements.txt (line 13)) (0.18.2)\n",
      "Requirement already satisfied: prance<24.0.0.0,>=23.6.21.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from semantic_kernel->-r ../src/requirements.txt (line 13)) (23.6.21.0)\n",
      "Requirement already satisfied: regex<2024.0.0,>=2023.6.3 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from semantic_kernel->-r ../src/requirements.txt (line 13)) (2023.12.25)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure.storage.blob->-r ../src/requirements.txt (line 14)) (42.0.5)\n",
      "Requirement already satisfied: msrest>=0.6.21 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from azure.ai.formrecognizer->-r ../src/requirements.txt (line 15)) (0.7.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from aiohttp>=3.6.2->azure-functions-durable->-r ../src/requirements.txt (line 9)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from aiohttp>=3.6.2->azure-functions-durable->-r ../src/requirements.txt (line 9)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from aiohttp>=3.6.2->azure-functions-durable->-r ../src/requirements.txt (line 9)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from aiohttp>=3.6.2->azure-functions-durable->-r ../src/requirements.txt (line 9)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from aiohttp>=3.6.2->azure-functions-durable->-r ../src/requirements.txt (line 9)) (1.9.4)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from cryptography>=2.1.4->azure.storage.blob->-r ../src/requirements.txt (line 14)) (1.16.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r ../src/requirements.txt (line 10)) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r ../src/requirements.txt (line 10)) (0.9.0)\n",
      "Requirement already satisfied: orderedmultidict>=1.0.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from furl>=2.1.0->azure-functions-durable->-r ../src/requirements.txt (line 9)) (1.0.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain->-r ../src/requirements.txt (line 10)) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain-core<0.2,>=0.1.29->langchain->-r ../src/requirements.txt (line 10)) (4.3.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langchain-core<0.2,>=0.1.29->langchain->-r ../src/requirements.txt (line 10)) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r ../src/requirements.txt (line 10)) (3.9.15)\n",
      "Requirement already satisfied: pymongo<5,>=4.5 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from motor<4.0.0,>=3.3.2->semantic_kernel->-r ../src/requirements.txt (line 13)) (4.6.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from msrest>=0.6.21->azure.ai.formrecognizer->-r ../src/requirements.txt (line 15)) (1.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openai>=1.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openai>=1.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openai>=1.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openai>=1.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (4.66.2)\n",
      "Requirement already satisfied: asgiref<4.0.0,>=3.6.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (3.7.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (4.21.1)\n",
      "Requirement already satisfied: jsonschema-spec<0.3.0,>=0.2.3 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.2.4)\n",
      "Requirement already satisfied: more-itertools in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (10.2.0)\n",
      "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.6.2)\n",
      "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.7.1)\n",
      "Requirement already satisfied: parse in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (1.20.1)\n",
      "Requirement already satisfied: werkzeug in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (3.0.1)\n",
      "Requirement already satisfied: chardet>=3.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (5.2.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.10 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.18.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain->-r ../src/requirements.txt (line 10)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain->-r ../src/requirements.txt (line 10)) (2.16.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->-r ../src/requirements.txt (line 10)) (3.0.3)\n",
      "Requirement already satisfied: pycparser in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from cffi>=1.12->cryptography>=2.1.4->azure.storage.blob->-r ../src/requirements.txt (line 14)) (2.21)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.18.0)\n",
      "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.4.3)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.1.4)\n",
      "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.3.2)\n",
      "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (1.10.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from pymongo<5,>=4.5->motor<4.0.0,>=3.3.2->semantic_kernel->-r ../src/requirements.txt (line 13)) (2.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure.ai.formrecognizer->-r ../src/requirements.txt (line 15)) (3.2.2)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from ruamel.yaml>=0.17.10->prance<24.0.0.0,>=23.6.21.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.2.8)\n",
      "Requirement already satisfied: colorama in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from tqdm>4->openai>=1.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->-r ../src/requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (from werkzeug->openapi_core<0.19.0,>=0.18.0->semantic_kernel->-r ../src/requirements.txt (line 13)) (2.1.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\github\\kagami\\src\\.venv\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../src/requirements.txt\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langdetect import detect_langs\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Azure AI multi-service endpoint:  https://cajetzeraiservices.cognitiveservices.azure.com/\n",
      "Using Azure AI multi-service key:  ********************************\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ai_multiservice_endpoint = os.environ.get(\"AI_MULTISERVICE_ENDPOINT\")\n",
    "ai_multiservice_key = os.environ.get(\"AI_MULTISERVICE_KEY\")\n",
    "\n",
    "print(\"Using Azure AI multi-service endpoint: \", ai_multiservice_endpoint)\n",
    "print(\"Using Azure AI multi-service key: \", len(ai_multiservice_key) * \"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ユーティリティ関数\n",
    "\n",
    "### validate_and_format_json\n",
    "C# の TryParse スタイルのメソッドと同様の方法で使用され、オブジェクトが JSON として解析可能かどうかをテストし、操作結果を示すフラグと結果の JSON または null 値の両方を含むタプルを提供します。JSONに対して有効な'''null'''値を取り戻す可能性がある場合、通常はブール値の値が関係してきますが、この場合、それは機能的に同じであり、この関数の使用意図であるOpen AIからのありそうもない結果であるため、JSONまたは'''null'''に減らすことができます。これは、Open AIがプロンプトエンジニアリングに基づいてJSON応答を生成したことを検証します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_format_json(json_string, indent=4):\n",
    "    try:\n",
    "        # Attempt to parse the JSON string\n",
    "        parsed_json = json.loads(json_string)\n",
    "        # If successful, re-encode it with indentation for pretty printing\n",
    "        return True, json.dumps(parsed_json, indent=indent)\n",
    "    except json.JSONDecodeError:\n",
    "        # If parsing fails, return an error message\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_text_to_boolean\n",
    "'''validate_and_format_json''' と同様に、これは Open AI 出力の検証を容易にすることを目的としています。この場合、ブール応答を取得しようとし、有効な応答が得られた場合はそれを提供し、それ以外の場合は null 値を提供します。これは、ブール応答のみに簡略化することもできますが、ダウンストリームのニーズに対する理解が不十分なため、ブール値の解析の失敗は否定的なテスト結果とは異なるものとして扱います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text_to_boolean(text):\n",
    "    if text.lower() == \"true\":\n",
    "        return True\n",
    "    elif text.lower() == \"false\":\n",
    "        return False\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate_mode_response\n",
    "これは、比較可能なJSONオブジェクトのリストを取得し、すべてのオブジェクトのプロパティでモード値を見つけようとする、より複雑なユーティリティ関数です。現在、わかりやすくするために、標準のスキーマを前提としています。現在、この関数で取り組んでいる2つの「バグ」があります\n",
    "1. リスト プロパティはハッシュ可能ではないため、モード応答を識別するために使用される応答の一時的なディクショナリのキーとして使用することはできません。リストの応答をフラット化し、リスト内の各値に対してそれらを応答に追加する必要があります\n",
    "2. この関数は現在、空の JSON オブジェクトを効果的に処理していません\n",
    "私は、これが主にPythonコードとの親密さの欠如のためにパフォーマンスを発揮しないのではないかと懸念しています。これはこの問題に取り組むための合理的な方法だと思いますが、論理的な観点からより創造的でパフォーマンスの高い解決を考え出す人を想像することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_mode_response(list_of_json_objects):\n",
    "#     print(list_of_json_objects)\n",
    "\n",
    "#     if len(list_of_json_objects) == 0:\n",
    "\n",
    "#         return None\n",
    "\n",
    "#     # remove any None objects from the list\n",
    "#     list_of_json_objects = [val for val in list_of_json_objects if val is not None]\n",
    "\n",
    "#     first_entry = json.loads(list_of_json_objects[0])\n",
    "\n",
    "#     property_keys = first_entry.keys()\n",
    "\n",
    "#     mode_response = {}\n",
    "\n",
    "#     responses = {}\n",
    "\n",
    "#     for key in property_keys:\n",
    "\n",
    "#         for obj in list_of_json_objects:\n",
    "#             if isinstance(obj, list):\n",
    "#                 print(f\"{obj} is a list in {list_of_json_objects}\", list_of_json_objects)\n",
    "\n",
    "#             json_obj = json.loads(obj)\n",
    "\n",
    "#             value = json_obj[key]\n",
    "\n",
    "#             # todo: determine how to handle list values, for now skipping to prepare for demo\n",
    "\n",
    "#             if isinstance(value, list):\n",
    "\n",
    "#                 continue\n",
    "\n",
    "#             if json_obj[key] in responses:\n",
    "\n",
    "#                 responses[value] += 1\n",
    "\n",
    "#             else:\n",
    "\n",
    "#                 responses[value] = 1\n",
    "\n",
    "#         if len(responses) > 0:\n",
    "\n",
    "#             mode_response[key] = max(responses, key=responses.get)\n",
    "\n",
    "#         responses = {}\n",
    "\n",
    "#     return mode_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatten_json\n",
    "テストを試行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json(json_obj):\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=\"\"):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + \"_\")\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + \"_\")\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(json_obj)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## チャンキング\n",
    "チャンキングのテクニックについては、chunking-research.ipynb で詳しく説明しています。ここでは、フローの残りの部分をテストする際に活用できるチャンクを取得しようとしています。 *注目すべき変更* ここで、それを関数に変え、uri_pathをファイルパスに変更したことです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_document(file_path):\n",
    "    ai_doc_intel_loader = AzureAIDocumentIntelligenceLoader(\n",
    "        file_path=file_path,\n",
    "        api_key=ai_multiservice_key,\n",
    "        api_endpoint=ai_multiservice_endpoint,\n",
    "        api_model=\"prebuilt-layout\",\n",
    "    )\n",
    "\n",
    "    docs = ai_doc_intel_loader.load()\n",
    "\n",
    "    # Split the document into chunks base on markdown headers.\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "\n",
    "    text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "    docs_string = docs[0].page_content\n",
    "\n",
    "    splits = text_splitter.split_text(docs_string)\n",
    "\n",
    "    chunks = []\n",
    "    \n",
    "    for split in splits:\n",
    "        chunks.append(split.page_content)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## イントロダクションとエンフォースメントチャンクの特定\n",
    "これは、より多くの作業と実験が必要となる領域です。マークダウンの周りで、AI関連ではない面倒なコーディングを行うことができるかもしれません。私は、各チャンクを1つの文に要約し、チャンクインデックスで文に番号を付け、AOAIにどの文が要約に属しているかを推測させるという、さまざまな結果を持つ手法を試しました。それは、より洗練されたプロンプトで機能できると思います。今のところ、より素朴なソリューションで実行しています"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_introduction_chunks(chunks):\n",
    "    intro_chunks = []\n",
    "    intro_chunks.append(chunks[0])  \n",
    "    return intro_chunks\n",
    "\n",
    "def identify_conclusion_chunks(chunks):\n",
    "    conclusion_chunks = []\n",
    "    conclusion_chunks.append(chunks[-1])\n",
    "    return conclusion_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セマンティックカーネルのセットアップ\n",
    "....\n",
    "- プラグインディレクトリを変更して、プロジェクト内のプラグインコードから実際に借用し、ノートブックと機能の間のパリティを維持することを容易にする他のコードの一部でこれを試みることができますが、コードを並べて簡単に説明できる利点が失われます...\n",
    "\n",
    "todo: functionalize and return kernel\n",
    "\n",
    "```__file__``` Jupyter Notebook では利用できないため、os.getcwd を使用します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using aoai_deployment: gpt-35-turbo-16k-deployment\n",
      "Using aoai_endpoint: https://exp-aoai.openai.azure.com/\n",
      "Using aoai_key: **********************************\n"
     ]
    }
   ],
   "source": [
    "curr_directory = os.getcwd()\n",
    "plugins_directory = os.path.join(curr_directory, \"..\\\\src\\\\plugins\")\n",
    "\n",
    "# setup semantic kernel\n",
    "aoai_deployment = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "aoai_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "print(f\"Using aoai_deployment: {aoai_deployment}\")\n",
    "print(f\"Using aoai_endpoint: {aoai_endpoint}\")\n",
    "print(f\"Using aoai_key: {len(aoai_endpoint)*'*'}\")\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "service_id = \"default\"\n",
    "\n",
    "service = AzureChatCompletion(\n",
    "    service_id=service_id,\n",
    "    deployment_name=aoai_deployment,\n",
    "    endpoint=aoai_endpoint,\n",
    "    api_key=aoai_key,\n",
    ")\n",
    "\n",
    "kernel.add_service(service)\n",
    "\n",
    "plugin_names = [\n",
    "    plugin\n",
    "    for plugin in os.listdir(plugins_directory)\n",
    "    if os.path.isdir(os.path.join(plugins_directory, plugin))\n",
    "]\n",
    "\n",
    "# for each plugin, add the plugin to the kernel\n",
    "try:\n",
    "    for plugin_name in plugin_names:\n",
    "        kernel.import_plugin_from_prompt_directory(plugins_directory, plugin_name)\n",
    "except ValueError as e:\n",
    "    logging.exception(f\"Plugin {plugin_name} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ドキュメントの分類\n",
    "これは、導入と結論を評価するだけでコストを節約しながら、比較的正確な結果を得ることができると信じています。時折、正確な結果が得られることがあります。大きなドキュメントの場合は、チャンクを結合できない可能性があるため、各チャンクに対してプラグインを呼び出し、結果のモードを見つけるように修正する必要があるかもしれません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def classify_study_type(chunks, kernel):\n",
    "    joined_chunks = f\"{os.linesep}\".join(chunks)\n",
    "    skresult = await kernel.invoke(\n",
    "        kernel.plugins[\"ClassificationPlugin\"][\"ClassifyStudyType\"],\n",
    "        sk.KernelArguments(input=joined_chunks,tense=\"present\"),\n",
    "    )\n",
    "    _, intro_study_type_classification = validate_and_format_json(\n",
    "        skresult.value[0].content, None\n",
    "    )\n",
    "    return intro_study_type_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## センチメント分析\n",
    "ここでは、調査結果が決定的で重要であるかどうかを判断しようと試みますが、これも導入チャンクを使用して実行できると信じています。分類と同様に、チャンクが大きすぎて大きなドキュメントには対応できない場合は、チャンクごとに実行し、結果を平均化する必要があるかもしれません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def are_findings_conclusive_and_signficant(chunks, kernel):\n",
    "    joined_chunks = f\"{os.linesep}\".join(chunks)\n",
    "    skresult = await kernel.invoke(\n",
    "        kernel.plugins[\"SummaryPlugin\"][\"AreStudyFindingsSignificant\"],\n",
    "        sk.KernelArguments(input=joined_chunks),\n",
    "    )\n",
    "    return parse_text_to_boolean(skresult.value[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンティティの抽出\n",
    "ここからが忙しくなるところです。ステークホルダー、日付、そしてすべてのチャンクに対して実行しているマルチエンティティ抽出関数があり、可能性を見逃さないようにし、結果のモードを見つけます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mode_response(list_of_json_objects, printObjs=False):\n",
    "    if len(list_of_json_objects) == 0:\n",
    "        return None\n",
    "\n",
    "    # remove any None objects from the list\n",
    "    list_of_json_objects = [val for val in list_of_json_objects if val is not None]\n",
    "\n",
    "    if printObjs:\n",
    "        print(\"List of JSON Objects: \", list_of_json_objects)\n",
    "\n",
    "    first_entry = list_of_json_objects[0]\n",
    "    # hacky conversion of other types to dictionary for keys\n",
    "    schema = (\n",
    "        json.loads(first_entry)\n",
    "        if isinstance(first_entry, str)\n",
    "        else json.loads(json.dumps(first_entry))\n",
    "    )\n",
    "\n",
    "    if printObjs:\n",
    "        print(\"Schema: \", schema)\n",
    "\n",
    "    property_keys = schema.keys()\n",
    "\n",
    "    mode_response = {}\n",
    "    responses = {}\n",
    "\n",
    "    for key in property_keys:\n",
    "        if printObjs:\n",
    "            print(\"Beginning loop for key: \", key)\n",
    "        for obj in list_of_json_objects:\n",
    "            if printObjs:\n",
    "                print(\"Evaluating: \", obj)\n",
    "\n",
    "            # hacky conversion of other types to dictionary for keysl see above\n",
    "            json_obj = (\n",
    "                json.loads(obj) if isinstance(obj, str) else json.loads(json.dumps(obj))\n",
    "            )\n",
    "\n",
    "            flatten_json(json_obj)\n",
    "            if isinstance(obj, list):\n",
    "                print(\n",
    "                    f\"{obj} is a list in {list_of_json_objects}\", list_of_json_objects\n",
    "                )\n",
    "\n",
    "            value = json_obj[key]\n",
    "\n",
    "            # todo: determine how to handle list values, for now skipping to prepare for demo\n",
    "\n",
    "            if isinstance(value, list):\n",
    "                continue\n",
    "\n",
    "            if json_obj[key] in responses:\n",
    "                responses[value] += 1\n",
    "            else:\n",
    "                responses[value] = 1\n",
    "\n",
    "        if len(responses) > 0:\n",
    "            mode_response[key] = max(responses, key=responses.get)\n",
    "        responses = {}\n",
    "    return mode_response\n",
    "\n",
    "\n",
    "# for lists like this ['[{\"a\": \"b\"}, {\"c\": \"d\"}]', '[{\"e\": \"f\"}, {\"g\": \"h\"}]'] -> [{\"a\": \"b\"}, {\"c\": \"d\"}, {\"e\": \"f\"}, {\"g\": \"h\"}]\n",
    "def flatten_list_of_list_strings(original_list):\n",
    "    list_of_lists = [json.loads(val) for val in original_list if val is not None]\n",
    "    return [val for sublist in list_of_lists for val in sublist]\n",
    "\n",
    "\n",
    "###\n",
    "async def use_extraction_function(chunk, function_name, kernel):\n",
    "    extract_entities_result = await kernel.invoke(\n",
    "        kernel.plugins[\"EntityExtraction\"][function_name],\n",
    "        sk.KernelArguments(input=chunk),\n",
    "    )\n",
    "    # print(extract_entities_result.value[0].content)\n",
    "    _, extracted_entities = validate_and_format_json(\n",
    "        extract_entities_result.value[0].content, None\n",
    "    )\n",
    "\n",
    "    return extracted_entities\n",
    "\n",
    "\n",
    "\n",
    "async def extract_entities(chunks, kernel):\n",
    "    extracted_entity_responses = []\n",
    "    extracted_stakeholders_responses = []\n",
    "    extracted_dates_responses = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        extract_entities = await use_extraction_function(\n",
    "            chunk, \"ExtractMultipleEntities\", kernel\n",
    "        )\n",
    "        extracted_entity_responses.append(extract_entities)\n",
    "        extracted_stakeholders = await use_extraction_function(\n",
    "            chunk, \"ExtractStakeholders\", kernel\n",
    "        )\n",
    "        extracted_stakeholders_responses.append(extracted_stakeholders)\n",
    "        extracted_dates = await use_extraction_function(\n",
    "            chunk, \"ExtractSignificantDates\", kernel\n",
    "        )\n",
    "        extracted_dates_responses.append(extracted_dates)\n",
    "\n",
    "\n",
    "    # mode entity extraction results across chunks\n",
    "\n",
    "    # multi entities\n",
    "    mode_extracted_entities_response = calculate_mode_response(\n",
    "        extracted_entity_responses\n",
    "    )\n",
    "\n",
    "    # stakeholders\n",
    "    extracted_stakeholders_responses = flatten_list_of_list_strings(\n",
    "        extracted_stakeholders_responses\n",
    "    )\n",
    "    mode_extracted_stakeholders_response = calculate_mode_response(\n",
    "        extracted_stakeholders_responses\n",
    "    )\n",
    "\n",
    "    # dates\n",
    "    extracted_dates_responses = flatten_list_of_list_strings(extracted_dates_responses)\n",
    "    mode_extracted_dates_response = calculate_mode_response(\n",
    "        extracted_dates_responses\n",
    "    )\n",
    "\n",
    "\n",
    "    deep_final_result = {\n",
    "        \"avg_extracted_entities_response\": mode_extracted_entities_response,\n",
    "        \"avg_extracted_stakeholders_response\": mode_extracted_stakeholders_response,\n",
    "        \"avg_extracted_dates_response\": mode_extracted_dates_response,\n",
    "    }\n",
    "\n",
    "\n",
    "    flat_final_result = flatten_json(deep_final_result)\n",
    "\n",
    "    return flat_final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## フローをつなぐ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating against document: c:\\Projects\\kagami\\notebooks\\..\\sample-docs\\2 - jbm-5-015.pdf\n",
      "Processing chunk 0\n",
      "Processing chunk 1\n",
      "Processing chunk 2\n",
      "Processing chunk 3\n",
      "Processing chunk 4\n",
      "Processing chunk 5\n",
      "Processing chunk 6\n",
      "Processing chunk 7\n",
      "Processing chunk 8\n",
      "Processing chunk 9\n",
      "Processing chunk 10\n",
      "Processing chunk 11\n",
      "Processing chunk 12\n",
      "Processing chunk 13\n",
      "Processing chunk 14\n",
      "Processing chunk 15\n",
      "Processing chunk 16\n",
      "Processing chunk 17\n",
      "Processing chunk 18\n",
      "Processing chunk 19\n",
      "{\n",
      "    \"entities_avg_extracted_entities_response_Duration\": \"\",\n",
      "    \"entities_avg_extracted_entities_response_DrugOrCompound\": \"\",\n",
      "    \"entities_avg_extracted_entities_response_RouteOfAdministration\": \"\",\n",
      "    \"entities_avg_extracted_entities_response_InternalStudyNumber\": \"\",\n",
      "    \"entities_avg_extracted_entities_response_ExternalStudyNumber\": \"\",\n",
      "    \"entities_avg_extracted_entities_response_TestFacility\": \"\",\n",
      "    \"entities_avg_extracted_stakeholders_response_name\": \"methylprednisolone\",\n",
      "    \"entities_avg_extracted_stakeholders_response_contact\": \"\",\n",
      "    \"entities_avg_extracted_dates_response_date\": \"2014\",\n",
      "    \"languages_0_language\": \"en\",\n",
      "    \"languages_0_probability\": 0.9999948169844857,\n",
      "    \"are_findings_significant\": false,\n",
      "    \"study_type\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# todo: read document names from sample docs directory and then loop through\n",
    "\n",
    "curr_directory = os.getcwd()\n",
    "sample_docs_directory = os.path.join(curr_directory, \"..\\\\sample-docs\")\n",
    "dir_list = os.listdir(sample_docs_directory)\n",
    "\n",
    "file_path = os.path.join(sample_docs_directory, dir_list[0])\n",
    "print(f\"Operating against document: {file_path}\")\n",
    "\n",
    "chunks = chunk_document(file_path)\n",
    "intro_chunks = identify_introduction_chunks(chunks)\n",
    "conclusion_chunks = identify_conclusion_chunks(chunks)\n",
    "\n",
    "combined_intro_conclusion_chunks = intro_chunks + conclusion_chunks\n",
    "\n",
    "study_type = await classify_study_type(combined_intro_conclusion_chunks, kernel)\n",
    "\n",
    "are_findings_significant = await are_findings_conclusive_and_signficant(\n",
    "    combined_intro_conclusion_chunks, kernel\n",
    ")  # potentially just use conclusion chunks\n",
    "\n",
    "# right now, we are just using the first chunk to determine the language, depending on business case you could use all chunks in the event it is a multi-language document you'll be more likely to get a more accurate result\n",
    "languages_result = detect_langs(intro_chunks[0])\n",
    "languages = []\n",
    "for lang in languages_result:\n",
    "    languages.append({\"language\": lang.lang, \"probability\": lang.prob})\n",
    "\n",
    "entities = await extract_entities(chunks, kernel)\n",
    "\n",
    "deep_final_result = {\n",
    "    \"entities\": entities,\n",
    "    \"languages\": languages,\n",
    "    \"are_findings_significant\": are_findings_significant,\n",
    "    \"study_type\": study_type,\n",
    "}\n",
    "\n",
    "flat_final_result = flatten_json(deep_final_result)\n",
    "\n",
    "print(json.dumps(flat_final_result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check_for_handwritten_signature\n",
    "Document Intelligence を使用して、ドキュメントのチャンク内の手書きの署名を検出する機能をテストする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating against document: c:\\GitHub\\kagami\\notebooks\\..\\sample-docs\\DLM_Window_Quote_Jan21_signed.pdf\n",
      "has_handwritten_sig = True\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "curr_directory = os.getcwd()\n",
    "sample_docs_directory = os.path.join(curr_directory, \"..\\\\sample-docs\")\n",
    "dir_list = os.listdir(sample_docs_directory)\n",
    "\n",
    "file_path = os.path.join(sample_docs_directory, dir_list[0])\n",
    "print(f\"Operating against document: {file_path}\")\n",
    "\n",
    "# Replace 'ai_multiservice_key' and 'ai_multiservice_endpoint' with your actual credentials\n",
    "\n",
    "def check_for_handwritten_style(file_path):\n",
    "    # Extract the text chunk from the arguments\n",
    "    chunks = chunk_document(file_path)\n",
    "\n",
    "    # Use the Document Intelligence API to analyze the text in the last chunk\n",
    " \n",
    "    text_chunk = chunks[-1]\n",
    "    # api_key = os.environ.get(\"AZURE_FORM_RECOGNIZER_API_KEY\")\n",
    "    # endpoint = os.environ.get(\"AZURE_FORM_RECOGNIZER_ENDPOINT\")\n",
    "    credential = AzureKeyCredential(ai_multiservice_key)\n",
    "    client = DocumentIntelligenceClient(endpoint=ai_multiservice_endpoint, credential=credential)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        poller = client.begin_analyze_document(\n",
    "            \"prebuilt-layout\", analyze_request=f, content_type=\"application/octet-stream\"\n",
    "        )\n",
    "    result: AnalyzeResult = poller.result()\n",
    "\n",
    "    if result.styles and any([style.is_handwritten for style in result.styles]):\n",
    "        return True\n",
    "    else: return False\n",
    "        # result = client.begin_analyze_document(text_chunk).result()\n",
    "\n",
    "    # Check if the text is handwritten style\n",
    "    # is_handwritten_style = result.isHandwritten()\n",
    "    # return is_handwritten_style\n",
    "\n",
    "has_handwritten_sig = check_for_handwritten_style(file_path)\n",
    "print(\"has_handwritten_sig = \" + str(has_handwritten_sig))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
